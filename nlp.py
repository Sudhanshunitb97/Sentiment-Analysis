# -*- coding: utf-8 -*-
"""nlp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vXvm_xOySli01sJtG_Ah8pRjSkX0m8eL
"""

import pandas as pd
import numpy as np

data=pd.read_csv("/content/jee_coaching_reviews.csv")

d=pd.read_csv("/content/positive_reviews_50000.csv")

k=pd.read_csv("/content/coaching_reviews_combined_50000.csv")

df=pd.concat([data, d,k], ignore_index=True)

df

df["review"].nunique()

import nltk
nltk.download('punkt_tab')
nltk.download('punkt')  # Sentence tokenizer
nltk.download('wordnet')  # Lemmatization
nltk.download('stopwords')  # Stop word

pip install contractions

import contractions
def remove_contractions(text):
  return contractions.fix(text)

df["review"]=df["review"].apply(remove_contractions)

!pip install symspellpy

from symspellpy import SymSpell, Verbosity

sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)

dictionary_path = "/content/frequency_dictionary_en_82_765.txt"


sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)
def spell_correct(text):

  corrected_text = " ".join([sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)[0].term if sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2) else word for word in text.split()])

  return corrected_text

df["review"]=df["review"].apply(spell_correct)

import string

def remove_punctuation(text):
    return text.translate(str.maketrans("", "", string.punctuation))

df["cleaned_text"] = df["review"].apply(remove_punctuation)

df

from nltk.tokenize import word_tokenize
df["words"]=df["cleaned_text"].apply(word_tokenize)

from nltk.corpus import stopwords

from nltk.stem import WordNetLemmatizer
lemmatizer=WordNetLemmatizer()

def stop(words):
  words=[lemmatizer.lemmatize(word).lower() for word in words if word not in stopwords.words("english")]
  return words

df["refined_words"]=df["words"].apply(stop)

df["refined_text"]=df["refined_words"].apply(lambda x: " ".join(x))

df

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df["refined_text"]).toarray()
y=df["rating"]
X

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict on test data
y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse:.5f}")

pip install --upgrade numpy

!pip uninstall gensim
!pip install gensim

!pip cache purge
!pip install --no-cache-dir numpy gensim

import gensim
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import nltk

# Train a Word2Vec model using the tokenized text
w2vmodel = Word2Vec(sentences=df['words'], vector_size=100, window=10, min_count=1, workers=4)

# Save the model for future use
w2vmodel.save("word2vec_feedback.model")

import numpy as np

def get_avg_vector(tokens, model):
    vectors = [model.wv[word] for word in tokens if word in model.wv]
    if vectors:
        return np.mean(vectors, axis=0)
    else:
        return np.zeros(model.vector_size)

X = df["words"].apply(lambda x: get_avg_vector(x, w2vmodel))
X = np.vstack(X.values)
y = df["rating"]

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model_2 = LinearRegression()
model_2.fit(X_train, y_train)
y_pred = model_2.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse:.5f}")

import re
def preprocess(text):
    # 1. Lowercase
    text = text.lower()

    # 2. Remove punctuation and non-alphabetic characters
    text = re.sub(r"[^a-zA-Z\s]", "", text)

    # 3. Tokenize
    tokens = word_tokenize(text)

    # 4. Remove stopwords
    tokens = [word for word in tokens]

    return tokens

def predict_rating(review):
    tokens = preprocess(review)
    vec = get_avg_vector(tokens, w2vmodel).reshape(1, -1)
    return model_2.predict(vec)[0]

print(predict_rating("helpful teachers"))

print(predict_rating("good amazing best teachers"))

import pickle

w2vmodel.save("w2vmodel_2.model")
with open("regressor_2.pkl", "wb") as f:
    pickle.dump(model_2, f)

print("‚úÖ Models trained and saved successfully!")

pip install streamlit

w2vmodel = Word2Vec.load("w2vmodel_2.model")

import streamlit as st
import numpy as np
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import pickle
from gensim.models import Word2Vec

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
stop_words = set(stopwords.words("english"))

# --- Load Models ---
@st.cache_resource
def load_models():
    w2vmodel = Word2Vec.load("w2vmodel_2.model")
    with open("regressor_2.pkl", "rb") as f:
        regressor = pickle.load(f)
    return w2vmodel, regressor

w2vmodel, regressor = load_models()

# --- Preprocessing ---
def preprocess(text):
    text = text.lower()
    text = re.sub(r"[^a-zA-Z\s]", "", text)
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stop_words]
    return tokens

# --- Convert tokens to review vector ---
def get_avg_vector(tokens, model):
    vectors = [model.wv[word] for word in tokens if word in model.wv]
    if not vectors:
        return np.zeros(model.vector_size)
    return np.mean(vectors, axis=0)

# --- Streamlit UI ---
st.title("üîÆ JEE Coaching Review Rating Predictor")

user_input = st.text_area("üìù Enter your review about the coaching:")

if st.button("Predict Rating"):
    tokens = preprocess(user_input)
    review_vec = get_avg_vector(tokens, w2vmodel).reshape(1, -1)
    predicted_rating = regressor.predict(review_vec)[0]
    st.success(f"üéØ Predicted Rating: {predicted_rating:.2f} / 10")

##simulation

import numpy as np

# Step 1: Use your actual MSE
mse_actual = 5.41
rmse_actual = np.sqrt(mse_actual)

# Step 2: Simulate prediction error using RMSE
np.random.seed(42)
n = 200000  # total predictions
y_true = np.random.uniform(0, 10, n).round(2)
y_pred = y_true + np.random.normal(0, rmse_actual, n)
y_pred = np.clip(y_pred, 0, 10)  # ensure predicted ratings stay in 0‚Äì10 range

# Step 3: Define margin of accuracy (¬±1 rating point)
tolerance = 1.0
accurate_predictions = np.abs(y_true - y_pred) <= tolerance
percent_accurate = accurate_predictions.mean() * 100

# Step 4: Business impact adjustment
original_revenue_cr = 6.80  # in crores
adjusted_revenue_cr = original_revenue_cr * (percent_accurate / 100)

# Step 5: Print results
print(f"‚úÖ Mean Squared Error (Given): {mse_actual}")
print(f"üìè Root Mean Squared Error (RMSE): {rmse_actual:.3f}")
print(f"üéØ Accuracy within ¬±1 rating point: {percent_accurate:.2f}%")
print(f"üí∞ Original Estimated Revenue Impact: ‚Çπ{original_revenue_cr:.2f} Cr")
print(f"‚ö†Ô∏è Adjusted Revenue Impact (Reliable Predictions Only): ‚Çπ{adjusted_revenue_cr:.2f} Cr")